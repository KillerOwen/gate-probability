\begin{enumerate}[label=\thechapter.\arabic*,ref=\thechapter.\theenumi]
\item Let $\{-1, -\frac{1}{2}, 1, \frac{5}{2}, 3\}$ be a realization of a random sample of size $5$ from a population having $N\left(\frac{1}{2}, \sigma^2\right)$ distribution, where $\sigma > 0$ is an unknown parameter. Let $T$ be an unbiased estimator of $\sigma^2$ whose variance attains the Cramer-Rao lower bound. Then, based on the above data, the realized value of $T$ (rounded off to two decimal places) equals
\hfill (GATE ST 2023)
\input{2023/ST/63/main.tex}
\item Let $X$ be a random variable with probability density function
\begin{align}
\label{eq:22/2023/1}f(x;\lambda)&=
\begin{cases}
\frac{1}{\lambda}e^{-\frac{x}{\lambda}} & \text{if} x>0\\
0 & \text{otherwise}
\end{cases}
\end{align}
where $\lambda > 0$ is an unknown parameter. Let $Y_1, Y_2,...,Y_n$ be a random sample of
size $n$ from a population having the same distribution as $X^2$.If
\begin{align}
\label{eq:22/2023/2}\bar{Y} &= \frac{1}{n}\sum_{i=1}^n Y_i
\end{align}
then which of the following statements is true?
\begin{enumerate}
\item \label{eq:22/2023/3}$\sqrt{\frac{\bar{Y}}{2}} \text{is a method of moments estimator of}         \lambda$
\item $\sqrt{\bar{Y}} \text{is a method of moments estimator of}\lambda$
\item ${\frac{1}{2}\sqrt{\bar{Y}}} \text{is a method of moments estimator of }\lambda$
\item $2\sqrt{\bar{Y}} \text{is a method of moments estimator of} \lambda$
\hfill(GATE ST 2023)\\
\end{enumerate}
\input{2023/ST/22/main.tex}
\item Suppose from the estimation of a linear regression model
$$Y_i=\beta_0+\beta_1X_i+e_i$$
the residual sum of squares and the total sum of squares are obtained as 44 and 80, respectively. The value of coefficient of determination is \\ (round off to two decimal places).
\hfill (GATE XH 2023)
\input{2023/XH/63/main.tex}
\item  Consider the following regression model
\begin{center}
	$y_{k} = \alpha_{0} + \alpha_{1} \log_{e}k + \epsilon_{k}, \qquad k = 1,2,…,n,$\\
\end{center}
where $\epsilon_{k}$'s are independent and identically distributed random variables each
having probability density function $ f\brak{x} = \frac{1}{2} e^{-|x|}, x \in \mathbb{R}$. Then which one of
the following statements is true? 
\begin{enumerate}[label=(\Alph*)]
	\item The maximum likelihood estimator of $\alpha_{0}$ does not exist
	\item The maximum likelihood estimator of $\alpha_{1}$ does not exist
	\item The least squares estimator of $\alpha_{0}$ exists and is unique
	\item The least squares estimator of $\alpha_{1}$ exists, but it is not unique
\end{enumerate}
\hfill(GATE ST 2023)\\
\input{2023/ST/26/main.tex}
\item Consider the following regression model
\begin{align}
y_t={\alpha}_0+{\alpha}_1t+{\alpha}_2t^2+\epsilon_{t}, \qquad t = 1,2,…,n
\end{align}
where ${\alpha}_0$ , ${\alpha}_1$ and ${\alpha}_2$ are unknown parameters and $\epsilon_{t}$’s are independent and identically distributed random variables each having $\gauss{\mu}{1}$ distribution with $\mu$ unknown. Then which of the following statements is/are true?
\begin{enumerate}
\item{There exists an unbiased estimator of ${\alpha}_1$}
\item{There exists an unbiased estimator of ${\alpha}_2$}
\item{There exists an unbiased estimator of ${\alpha}_0$}
\item{There exists an unbiased estimator of ${\mu}$}
\end{enumerate}
\hfill(GATE ST 2023)\\
\input{2023/ST/58/main.tex}
\item Let $X_1, X_2,...,X_n$ be a random sample of size $n$ from a population having uniform distribution over the interval $\brak{\frac{1}{3},\theta}$, where $\theta>\frac{1}{3}$ is an unknown parameter. If $Y = $ max \{$X_1, X_2,...,X_n$\}, then which one of the following statements is true?
\begin{enumerate}
\item $\brak{\frac{n+1}{n}}\brak{Y-\frac{1}{3}} + \frac{1}{3}$ is an unbiased estimator of $\theta$
\item $\brak{\frac{n}{n+1}}\brak{Y-\frac{1}{3}} + \frac{1}{3}$ is an unbiased estimator of $\theta$
\item $\brak{\frac{n+1}{n}}\brak{Y+\frac{1}{3}} - \frac{1}{3}$ is an unbiased estimator of $\theta$
\item $Y$ is an unbiased estimator of $\theta$
\end{enumerate} 
\hfill(GATE ST 2023)\\
\input{2023/ST/24/gate.tex}
\item Let $X_1$, $X_2$, ... , $X_n$ be a random sample of size $n$ from a population having probability density function
\begin{align}
p_X(x; \mu) =
\begin{cases}
e^{-(x-\mu)}, & \text{if } \mu \leq x < \infty \\
0, & \text{otherwise,} 
\end{cases}
\end{align}
where $\mu \in \mathbb{R}$ is an unknown parameter. If $\hat{M}$ is the maximum likelihood estimator of the median of $X_1$, then which one of the following statements is true?
\begin{enumerate}[label=\Alph*)]
  \item $\pr{\hat{M} \leq 2}$ = $1 - e^{-n(1-\log_e 2)}$ if $\mu = 1$
  \item $\pr{\hat{M} \leq 1}$ = $1 - e^{-n \log_e 2}$ if $\mu = 1$
  \item $\pr{\hat{M} \leq 3}$ = $1 - e^{-n(1-\log_e 2)}$ if $\mu = 1$
  \item $\pr{\hat{M} \leq 4}$ = $1 - e^{-n(2\log_e 2-1)}$ if $\mu = 1$
\end{enumerate}
\hfill(GATE ST 2023)\\
\input{2023/ST/39/main.tex}
\item Let $X_1, X_2, ..., X_{10}$ be a random sample of size 10 from a population having $\gauss{0}{\theta^2}$ distribution, where $\theta > 0$ is an unknown parameter.
Let $T = \frac{1}{10}\sum^{10}_{i=1}{X_i^2}$. If the mean square error of $cT \brak{c > 0}$ as an estimator of $\theta^2$, is minimized at $c = c_0$, then the value of $c_0$ equals
\begin{enumerate}[label =(\roman*)]
	\item $\frac{5}{6}$ \vspace{2pt}
	\item $\frac{2}{3}$ \vspace{2pt}
	\item $\frac{3}{5}$ \vspace{2pt}
	\item $\frac{1}{2}$ \vspace{2pt}
\end{enumerate}
\hfill{(GATE ST 2023)}\\
\input{2023/ST/40/main.tex}
\end{enumerate}
